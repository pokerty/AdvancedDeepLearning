{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import ssl\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 10),        # 10 output classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 2 * 2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model=AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_model_loss = 5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "      \n",
    "            prediction = model.forward(data)\n",
    "\n",
    "            train_loss = criterion(prediction, labels)\n",
    "\n",
    "            train_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "        print(f'\\rEpoch {epoch+1}, batch {i+1}/{len(train_loader)} - Loss: {train_loss}')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        writer.add_scalar(\"Loss/train ADAM\", train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        for batch_nr, (data, labels) in enumerate(val_loader):\n",
    "            prediction = model.forward(data)\n",
    "            loss_val = criterion(prediction, labels)\n",
    "            valid_losses.append(loss_val)\n",
    "        print(f\"loss validation: {loss_val}\")\n",
    "        #print(f\"loss validation: {loss_val}\",\"\\n\")\n",
    "\n",
    "        if valid_losses[-1] < best_model_loss:\n",
    "            print(f\"\\t > Found a better model, {best_model_loss} -> {valid_losses[-1]}\")\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_loss = valid_losses[-1]\n",
    "\n",
    "        writer.add_scalar(\"Loss/validation ADAM\", loss_val, epoch)\n",
    "\n",
    "    print(f\"\\nBest model loss: {best_model_loss}\")\n",
    "    return best_model, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(network, loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        for x, (data, labels) in enumerate(loader):\n",
    "\n",
    "            prediction = network.forward(data)\n",
    "\n",
    "            for i in range(len(data)):\n",
    "\n",
    "                y_true.append(labels[i].item())\n",
    "                y_pred.append(torch.argmax(prediction[i]).item())\n",
    "                if y_true[i] == y_pred[i]:\n",
    "                    correct += 1        \n",
    "    \n",
    "            total += float(len(data))\n",
    "    \n",
    "        score = correct/total\n",
    "\n",
    "        accuracy = score\n",
    "\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1, batch 40/40 - Loss: 1.855844259262085\n",
      "loss validation: 1.964345932006836\n",
      "\t > Found a better model, 5 -> 1.964345932006836\n",
      "Epoch 2, batch 40/40 - Loss: 1.5980778932571411\n",
      "loss validation: 1.5825409889221191\n",
      "\t > Found a better model, 1.964345932006836 -> 1.5825409889221191\n",
      "Epoch 3, batch 40/40 - Loss: 1.4516880512237549\n",
      "loss validation: 1.4356021881103516\n",
      "\t > Found a better model, 1.5825409889221191 -> 1.4356021881103516\n",
      "Epoch 4, batch 40/40 - Loss: 1.2556418180465698\n",
      "loss validation: 1.2733981609344482\n",
      "\t > Found a better model, 1.4356021881103516 -> 1.2733981609344482\n",
      "Epoch 5, batch 40/40 - Loss: 1.1666392087936401\n",
      "loss validation: 1.174381971359253\n",
      "\t > Found a better model, 1.2733981609344482 -> 1.174381971359253\n",
      "\n",
      "Best model loss: 1.174381971359253\n",
      "Model Accuracy (CIFAR10): 57.199999999999996%\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1000\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
    "\n",
    "validset, trainset = torch.utils.data.random_split(trainset, [10000, 40000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Loss function , Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Training\n",
    "trained_model, train_loss, valid_loss = train_model(model, criterion, optimizer, trainloader, validloader, EPOCHS)\n",
    "\n",
    "# Testing\n",
    "test_acc = get_accuracy(trained_model, testloader)\n",
    "print(f\"Model Accuracy (CIFAR10): {test_acc*100}%\")\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    model,\n",
    "    nn.Linear(10, 10))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 40/40 - Loss: 1.0603957176208496\n",
      "loss validation: 1.1493136882781982\n",
      "\t > Found a better model, 5 -> 1.1493136882781982\n",
      "Epoch 2, batch 40/40 - Loss: 1.0956660509109497\n",
      "loss validation: 1.1575943231582642\n",
      "Epoch 3, batch 40/40 - Loss: 1.084247350692749\n",
      "loss validation: 1.100934624671936\n",
      "\t > Found a better model, 1.1493136882781982 -> 1.100934624671936\n",
      "Epoch 4, batch 40/40 - Loss: 1.052489161491394\n",
      "loss validation: 1.2534207105636597\n",
      "Epoch 5, batch 40/40 - Loss: 1.1280475854873657\n",
      "loss validation: 1.2305376529693604\n",
      "\n",
      "Best model loss: 1.100934624671936\n",
      "Model Accuracy (AlexNet PreTrained): 57.099999999999994%\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "num_classes=10\n",
    "model_featext=trained_model\n",
    "\n",
    "\n",
    "# Freeze all layers except the last few layers\n",
    "for name, param in model_featext.named_parameters():\n",
    "    if \"Sequential.1\" in name or \"classifier.6\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "trained_model_featext, train_loss, valid_loss = train_model(model_featext, criterion, optimizer, trainloader, validloader, EPOCHS)\n",
    "\n",
    "\n",
    "test_acc = get_accuracy(trained_model_featext, testloader)\n",
    "print(f\"Model Accuracy (AlexNet PreTrained): {test_acc*100}%\")\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
